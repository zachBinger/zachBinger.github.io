<!DOCTYPE HTML>
<!--
	Portfolio by ZMB
-->
<html>
	<head>
		<title>NREL Project</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/prism.css"/>
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">HOME</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Main</a></li>
							<li class="active"><a href="generic.html">Publications</a></li>
							<li><a href="elements.html">Software</a></li>
							<li><a href="aboutMe.html">My Journey</a></li>
						</ul>
						<ul class="icons">
							<li><a href="#https://scholar.google.com/citations?user=lYbVN8oAAAAJ&hl=en&oi=sra" class="icon brands fa-google"><span class="label">Google Scholar</span></a></li>
							<li><a href="#https://www.linkedin.com/in/zbinger/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="#https://github.com/zachBinger" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
						<section class="post">
							<header class="major">
								<h1>NREL Job Search<br />
								2020</h1>
								<p>Creating a webscraper to see what kind of degrees NREL is looking for<br>FYI if you're not interested in the code and process, scroll down until you see pretty figures</p>
							</header>
							
							<p>For the past few years, the National Renewable Energy Laboratories (NREL) has stood out as a potential target for after finishing my schooling.  
								After working at Sandia National Labs in undergrad, I enjoyed the style of work done in the National Lab setting. It wasn't exactly like Industry 
								and it wasn't exactly like academia either.  Instead it seemed like the best of both. So in the interest of crafting my skills towards becoming an
								exemplary candidate, I decided to do some investigation of what degrees are most requested on NREL's job application webpage.  I'm going to start with 
								the project results, but there is a semi-walkthrough with embedded code after the presented figures</p>

							<a href="images/degreeFreq.png" class="image right"><img src="images/degreeFreq.png" alt="" /></a>
							<p>I found it interesting that, in a research lab full of scientist and engineers, the most requested degree type was Computer Science
								It makes sense since we live in a time period that some coin the "Age of Data".  <em>I mean, I'm making a data science portfolio for a reason I guess.</em>
								Following Computer Science are the degrees I expected to be most common. Physics, MechE, and Materials are the type of skill sets I was used to seeing at Sandia.
								Luckily my degree came in at a respectable 4<sup>th</sup> place. I was surprised however, that Chemistry fell into the bottom half, I did originally hypothesis that "Physical"
								was the 1<sup>st</sup> half of "Physical Chemisty" but that wouldn't matter because they still both occurred 6 times each, which means that they still only occured 6 times if
								they were originally paired together in job lisitngs
							</p>
							
							<p>Next was to satisfy the age old question: How many decades experience do I need? I love seeing Entry-level position postings that are like "14 years experience in X" expecially when
								the technology they want experience in has only existed for liek 6 years... just saying.
							</p>
							<!-- <a href="images/degreeFreq.png" class="image right"><img src="images/degreeFreq.png" alt="" /></a> -->

							<a href="images/workExp.png" class="image left"><img src="images/workExp.png" alt="" /></a>

							<p>This figure made me a little more confident that doing a PhD was a good investement.  On average 
								only 3 years experience are required for the many postings that required a PhD. There wasn't too much of a difference, on averaage, between a Master's and a Bachelor's degree, but I will
							note that during data scraping I saw a lot of double digit numbers for the bachelors, but they were offset by a few positions that required lower amounts such as 5 years. The listings for a 
							Master's for sure never exceeded 7 or 8 and absolutely never reached as high as 13 years experience like a Bachelor's degree</p>

							<p>These two figures answered my simple question:"what kind of degrees NREL is looking for?" and also showed be how much experience they want. For fun I threw in a couple
								Word Clouds that I shaped like the NREL logo The default is random colorized words on a white background, but the typography thickness doesn't do the contrast any favors so i also threw in
								a black and white variation which I think looks a little cleaner.  The word cloud was composed of the "action words" that were present in the "Additional" and "Preffered" requirements. Some
								popular words that NREL seems to feel like represents a good candidate are: Strong, Analysis, Technical, Research, ability, Problem Solving, Communication Skills, Experience, Team, etc..
							</p>

				
							<a href="images/wordCloudNREL.png" class="image fit"><img src="images/wordCloudNREL.png" alt="" /></a>
							<a href="images/wordCloudNREL_BW.png" class="image fit"><img src="images/wordCloudNREL_BW.png" alt="" /></a>



							<h1>Code Walkthrough</h1>


							<p>For the past few years, the National Renewable Energy Laboratories (NREL) has stood out as a potential target for after finishing my schooling.  
								After working at Sandia National Labs in undergrad, I enjoyed the style of work done in the National Lab setting. It wasn't exactly like Industry 
								and it wasn't exactly like academia either.  Instead it seemed like the best of both. So in the interest of crafting my skills towards becoming an
							exemplary candidate, I decided to do some investigation of what degrees are most requested on NREL's job application webpage.  To do this I need to do some webscraping</p>
							<p>My first idea was to crack out the BeautifulSoup library and extract some HTML and then make some bar charts to visualize the data. The following modules should suffice:</p>

							<pre><code class='language-python'
>from requests import get
from requests.exceptions import RequestException
from contextlib import closing
from bs4 import BeautifulSoup</code></pre>

							<p>However, the Career Listings page was very heaviliy dependant on Javascript and most of the test was hidden inside scripts. To get around this I decided to use Selenium which 
								would use Chrome to request and load the webpage before extracting the HTML</p>
											
							<pre><code class='language-python'
>from selenium import webdriver
import time
import pandas as pd
import pickle</code></pre>

							<p>Now that I had all the modules required for this task, i had to create some functions that would guide my Chrome browser to ask for the correct webpages for each job listing.  
								I started by finding the buttons to filter for the job types I was interested in (ie. Full Time and Post Doc positions). I then used BeautifulSoup to extract the HTML and collect
							all the Job IDs for later use.</p>

							<pre><code class='language-python'
>def getJobs(self):
regularJobs = self.driver.find_element_by_xpath('//*[@id="wd-FacetValue-CheckBox-workerSubType::41059f5be019104091d88ac710bc4319"]/div')
postDocJobs = self.driver.find_element_by_xpath('//*[@id="wd-FacetValue-CheckBox-workerSubType::41059f5be019104091d88f33e9ce431b"]/div')
regularJobs.click()
postDocJobs.click()
time.sleep(2)
soup = BeautifulSoup(self.driver.page_source, 'html.parser')
jobList = soup.findAll("span", class_="gwt-InlineLabel WK3F WJ2F")

jobIDs = list()
for job in range(len(jobList)):
	jobText = jobList[job].text
	jobIDs.append(jobText[:5])

self.driver.back()
return jobIDs</code></pre>

							<p>Now all Job IDs were stored in a list.  I then created a bot that would take each Job ID and use the search bar to isolate that job's page. Once on the jobs page I could then use BeautifulSoup
								to extract the HTML for that site to get that job's: Job Description, Basic Qualifications, Additional Required Qualifications, and Preffered Qualifications. However, at this stage I just pulled
								the whole page's HTML decided to clean it later
							</p>

							<pre><code class='language-python'
>def searchJob(self, jobID):
search_bar = self.driver.find_element_by_xpath('//*[@id="wd-AdvancedFacetedSearch-SearchTextBox-input"]')
search_bar.click()
search_bar.clear()
search_bar.send_keys(jobID)
search_btn = self.driver.find_element_by_xpath('//*[@id="wd-AdvancedFacetedSearch-facetSearchResult"]/div[1]/div[1]/div/button[2]')
search_btn.click()
time.sleep(2)
element = self.driver.find_element_by_xpath('//*[@data-automation-id="promptOption"]')
element.click()
time.sleep(2)

df = self.saveJob(jobID)
return pd.concat([master_df, df])</code></pre>

							<p>I'll spare you the following functions that actually find the text i'm looking for and cleans it up as it is quite cumbersome. But at this point the project is in a very good spot.  Now I have to
								store the text for each Job ID in a row of a DataFrame and store it for use with the code for the text analysis part of this project
							</p>

							<pre><code class='language-python'
>master_df = pd.DataFrame([['','','','']], columns=['Job Description', 'Basic Qualifications', 'Additional Required Qualifications', 'Preferred Qualifications'])

my_url = 'https://nrel.wd5.myworkdayjobs.com/NREL'
bot = nrelScraper()
bot.getSite(my_url)
jobIDs = bot.getJobs()
jobIDs.remove('R6295')  # This job didn't like getting scraped for some reason
time.sleep(2)
for idx, id in enumerate(jobIDs):
    try:
        print(id, idx/len(jobIDs))
        master_df = bot.searchJob(id)
        time.sleep(2)
    except:
        print("Problematic Job Id was:" + id)
        master_df.to_pickle("./master_df.pkl")
        time.sleep(5)
        master_df = bot.searchJob(id)

master_df.to_pickle("./master_df.pkl")</code></pre>

							<p>Now that I have all the data I need, relatively clean, I created a Jupyter Notebook to start doing some Natural Language analysis using NLTK. 
								I also want to make some bar chart and maybe a word cloud so i loaded up the standard visualization tools:</p>
							<pre><code class='language-python'
>import pandas as pd
import nltk
from nltk.util import ngrams
from word2number import w2n
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator</code></pre>

							<p>The NLTK library is relatively simple to use and fairly accurate, so I began by trusting that it could effectively nail down proper nouns that will represent the degree types.
								It did quite a good job and 
							</p>

							<pre><code class='language-python'
>def basicQuals(frame):
	degreeList = list()
	degreeExp = {'PhD':[],'Master':[],'Bachelor':[]}
	for row in frame.index:
		sentence = frame['Basic Qualifications'][row]
		tokens = nltk.word_tokenize(sentence)
		tagged = nltk.pos_tag(tokens)
		degreeList = degreeList + (getPropNouns(tagged))
		
	degreeList, degreeExp = cleanResults(degreeList)

	return degreeList, degreeExp

def addlQuals(frame):
	include_tags = ['JJ', 'NN', 'NNS', 'NNP', 'NNPS']
	full_tokens = list()
	for row in frame.index:
		sentence = frame['Additional Required Qualifications'][row].lower()
		tokens = nltk.word_tokenize(sentence)
		tokensSet = list(ngrams(tokens, 2)) # This was crucial for finding compound words
		tagged = nltk.pos_tag(tokens)
		filtered_tokens = [tok for tok, tag in tagged if tag in include_tags]
		full_tokens = full_tokens + filtered_tokens

	fdist1 = nltk.FreqDist(full_tokens)

	return fdist1, full_tokens</code></pre>
	
							<p>The data was now full ready for analysis. Since this is categorical data bar plots are the first figure type you should look at utilizing
								With very simple data such as this, it was the perfect choice and with the data stored in tuples i decided to unpack them into lists for easy
								use with the Seaborn library.  Now setting up the figure is straight forward 
							</p>

							<pre><code class='language-python'
>sns.set(style="whitegrid")
f, ax = plt.subplots(figsize=(10, 15))
sns.set_color_codes("pastel")
sns.barplot(x=xs, y=ys)

plt.ylabel(ylabel='Degree Topic',size=32)
plt.xlabel(xlabel='# Total Occurances\n in Job Listings',size=32)
plt.xticks(size=21)
plt.yticks(size=18)
plt.tight_layout()
</code></pre>
							
						</section>

					</div>

				<!-- Footer -->
				<footer id="footer">
					<section>
						<form method="post" action="#">
							<div class="fields">
								<div class="field">
									<label for="name">Name</label>
									<input type="text" name="name" id="name" />
								</div>
								<div class="field">
									<label for="email">Email</label>
									<input type="text" name="email" id="email" />
								</div>
								<div class="field">
									<label for="message">Message</label>
									<textarea name="message" id="message" rows="3"></textarea>
								</div>
							</div>
							<ul class="actions">
								<li><input type="submit" value="Send Message" /></li>
							</ul>
						</form>
					</section>
					<section class="split contact">
						<section>
							<h3>Phone</h3>
							<p><a href="#">(575) 418-1151</a></p>
						</section>
						<section>
							<h3>Email</h3>
							<p><a href="#">zacharybinger@icloud.com</a></p>
						</section>
						<section>
							<h3>Social</h3>
							<ul class="icons alt">
								<li><a href="#" class="icon brands alt fa-google"><span class="label">Google Scholar</span></a></li>
								<li><a href="#" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
								<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
					</section>
				</footer>

			<!-- Copyright -->
				<div id="copyright">
					<ul><li>&copy; ZMB LLC</li><li>Design: by ZMB 	- 	Hosted on: <a href="zachBinger.github.io">Github Pages</a></li></ul>
				</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="assets/js/prism.js"></script>

	</body>
</html>